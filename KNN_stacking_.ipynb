{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "KNN bagging & stacking"
      ],
      "metadata": {
        "id": "yOr4rXd7B2Av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is the fundamental idea behind ensemble techniques? How does\n",
        "bagging differ from boosting in terms of approach and objective?\n",
        "\n",
        "-The fundamental idea behind ensemble techniques is to combine multiple individual models (weak learners) to form a stronger, more accurate, and more stable model. The assumption is that a group of diverse models will outperform any single model.\n",
        "\n",
        "Difference Between Bagging and Boosting\n",
        "\n",
        "Aspect\tBagging (Bootstrap Aggregating)\tBoosting\n",
        "\n",
        "Approach\tTrains multiple models independently on different bootstrapped samples (random subsets of data).\tTrains models sequentially, where each new model focuses on correcting the errors of the previous one.\n",
        "Objective\tReduce variance and avoid overfitting by averaging predictions.\tReduce bias by creating a strong learner from several weak learners that focus on mistakes.\n",
        "Model Weighting\tAll models have equal weight.\tMisclassified samples get higher weight; later models get more influence.\n",
        "Examples\tRandom Forest\tAdaBoost, Gradient Boosting\n",
        "\n"
      ],
      "metadata": {
        "id": "uhjD4YSKB9JP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.: Explain how the Random Forest Classifier reduces overfitting compared to\n",
        "a single decision tree. Mention the role of two key hyperparameters in this process?\n",
        "\n",
        "-A single decision tree tends to overfit because it learns patterns too specific to the training data. A Random Forest, however, reduces overfitting by creating multiple decision trees using random subsets of data and features, then averaging their predictions. This randomness increases model diversity and improves generalization.\n",
        "\n",
        "Two Key Hyperparameters:\n",
        "\n",
        "1. n_estimators\n",
        "\n",
        "Number of trees in the forest.\n",
        "More trees reduce variance and improve stability.\n",
        "Helps in reducing overfitting by averaging many models.\n",
        "\n",
        "2. max_features\n",
        "\n",
        "Number of features considered at each split.\n",
        "Limiting features introduces randomness, reduces correlation among trees, and lowers overfitting.\n",
        "Other helpful hyperparameters: max_depth, min_samples_split, min_samples_leaf.\n",
        "\n"
      ],
      "metadata": {
        "id": "euqJQ6S4CeOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.: What is Stacking in ensemble learning? How does it differ from traditional\n",
        "bagging/boosting methods? Provide a simple example use case?\n",
        "\n",
        "-Stacking (Stacked Generalization) is an ensemble technique where predictions from multiple base models are used as input features for a meta-model (or final model), which makes the final prediction.\n",
        "\n",
        "How Stacking Differs:\n",
        "\n",
        "Bagging / Boosting\tStacking\n",
        "\n",
        "Combine predictions by voting or averaging.\tCombine predictions by training a meta-learner on model outputs.\n",
        "Focus on reducing variance (bagging) or bias (boosting).\tFocus on leveraging strengths of different model types.\n",
        "Uses many similar models.\tUses diverse models (e.g., SVM + Decision Tree + Logistic Regression).\n",
        "\n",
        "\n",
        "Simple Use Case:\n",
        "Predicting employee attrition by stacking:\n",
        "Base models: Random Forest, Logistic Regression, Gradient Boosting.\n",
        "Meta-model: Logistic Regression to learn how to best combine the predictions.\n",
        "This typically improves accuracy compared to using any single model.\n"
      ],
      "metadata": {
        "id": "h4qmowGiC3Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is the OOB Score in Random Forest, and why is it useful? How does\n",
        "it help in model evaluation without a separate validation set?\n",
        "\n",
        "-Grid Search with Random Forest is a hyperparameter tuning technique where Grid Search systematically tests multiple combinations of hyperparameters for a Random Forest model to find the best-performing configuration.\n",
        "\n",
        "Why It Is Useful:\n",
        "Random Forest has many hyperparameters (e.g., number of trees, depth, features per split).\n",
        "The right combination can significantly improve model accuracy and generalization.\n",
        "Grid Search automates the process of trying these combinations using cross-validation.\n",
        "\n",
        "How It Improves Model Performance\n",
        "\n",
        "1. Optimizes Hyperparameters:\n",
        "Grid Search tests combinations such as:\n",
        "n_estimators\n",
        "max_depth\n",
        "min_samples_split\n",
        "max_features\n",
        "It selects the combination that gives the best validation performance.\n",
        "\n",
        "2. Reduces Overfitting or Underfitting:\n",
        "If trees are too deep → model overfits.\n",
        "If trees are too shallow → model underfits.\n",
        "Grid Search finds the balance.\n",
        "\n",
        "3. Uses Cross-Validation for Stability:\n",
        "Evaluates each parameter set on multiple folds.\n",
        "Ensures the chosen model performs well on unseen data.\n"
      ],
      "metadata": {
        "id": "j2uAMzReDG42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Compare AdaBoost and Gradient Boosting in terms of:\n",
        "● How they handle errors from weak learners\n",
        "● Weight adjustment mechanism\n",
        "● Typical use cases\n",
        "\n",
        "-1. Handling Errors from Weak Learners\n",
        "\n",
        "AdaBoost:\n",
        "Focuses on misclassified samples by increasing their weights so that the next weak learner focuses more on these difficult cases.\n",
        "\n",
        "Gradient Boosting:\n",
        "Fits each new learner to the residual errors (the difference between predicted and actual values). It reduces error by moving in the direction of the negative gradient of the loss function.\n",
        "\n",
        "2. Weight Adjustment Mechanism\n",
        "\n",
        "AdaBoost:\n",
        "Misclassified samples → weight increases\n",
        "Correctly classified samples → weight decreases\n",
        "Each weak learner is assigned a weight based on its accuracy.\n",
        "Gradient Boosting:\n",
        "No sample weighting.\n",
        "Instead, each new learner tries to minimize a differentiable loss function using gradient descent.\n",
        "Tree predictions are scaled by the learning rate.\n",
        "\n",
        "3. Typical Use Cases\n",
        "\n",
        "AdaBoost:\n",
        "Suitable for clean, non-noisy datasets\n",
        "\n",
        "Binary classification, face detection, spam classification\n",
        "Works well with weak learners like small-depth decision trees\n",
        "Gradient Boosting:\n",
        "Regression & classification\n",
        "Complex datasets with non-linear relationships\n",
        "Real-world use: credit scoring, insurance risk, churn prediction\n",
        "More powerful and flexible than AdaBoost\n"
      ],
      "metadata": {
        "id": "37OBI0clEICv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Why does CatBoost perform well on categorical features without requiring\n",
        "extensive preprocessing? Briefly explain its handling of categorical variables?\n",
        "\n",
        "-CatBoost performs well on categorical features because it handles them internally using advanced encoding techniques, eliminating the need for manual preprocessing like One-Hot Encoding or Label Encoding.\n",
        "\n",
        "How CatBoost Handles Categorical Variables:\n",
        "\n",
        "1. Ordered Target Encoding (Target Statistics):\n",
        "CatBoost converts categorical values into numerical values by using statistics (mean target value) computed in an ordered fashion to avoid target leakage.\n",
        "2. Permutation-Based Processing:\n",
        "CatBoost applies random permutations to prevent the model from seeing future information, ensuring reliable encoding.\n",
        "3. Efficient Handling of High-Cardinality Features:\n",
        "It handles features with many categories (e.g., city names, product IDs) without memory explosion.\n",
        "4. Automatic Preprocessing:\n",
        "CatBoost automatically detects categorical features and applies the best encoding internally, making it fast and accurate.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1DhoOVHcEi_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7. KNN Classifier Assignment: Wine Dataset Analysis with\n",
        "#Optimization\n",
        "#Task:\n",
        "#1. Load the Wine dataset (sklearn.datasets.load_wine()).\n",
        "#2. Split data into 70% train and 30% test.\n",
        "#3. Train a KNN classifier (default K=5) without scaling and evaluate using:\n",
        "#a. Accuracy\n",
        "#b. Precision, Recall, F1-Score (print classification report)\n",
        "#4. Apply StandardScaler, retrain KNN, and compare metrics.\n",
        "#5. Use GridSearchCV to find the best K (test K=1 to 20) and distance metric\n",
        "#(Euclidean, Manhattan)\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 1. Load the Wine Dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split into train (70%) and test (30%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Unscaled KNN\n",
        "# ---------------------------------------------------------\n",
        "knn_default = KNeighborsClassifier()\n",
        "knn_default.fit(X_train, y_train)\n",
        "pred_unscaled = knn_default.predict(X_test)\n",
        "\n",
        "print(\"Accuracy (Unscaled KNN):\", accuracy_score(y_test, pred_unscaled))\n",
        "print(\"\\nClassification Report (Unscaled KNN):\\n\")\n",
        "print(classification_report(y_test, pred_unscaled))\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Scaling (StandardScaler)\n",
        "# ---------------------------------------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier()\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nAccuracy (Scaled KNN):\", accuracy_score(y_test, pred_scaled))\n",
        "print(\"\\nClassification Report (Scaled KNN):\\n\")\n",
        "print(classification_report(y_test, pred_scaled))\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# GridSearchCV Optimization\n",
        "# k = 1 to 20\n",
        "# Distance metrics: Euclidean, Manhattan\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "param_grid = {\n",
        "    \"n_neighbors\": list(range(1, 21)),\n",
        "    \"metric\": [\"euclidean\", \"manhattan\"]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nBest Parameters from GridSearchCV:\", grid.best_params_)\n",
        "\n",
        "# Evaluate optimized model\n",
        "best_knn = grid.best_estimator_\n",
        "pred_best = best_knn.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nAccuracy (Optimized KNN):\", accuracy_score(y_test, pred_best))\n",
        "print(\"\\nClassification Report (Optimized KNN):\\n\")\n",
        "print(classification_report(y_test, pred_best))"
      ],
      "metadata": {
        "id": "_YdvUpgTGLsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8.PCA + KNN with Variance Analysis and Visualization\n",
        "#Task:\n",
        "#1. Load the Breast Cancer dataset (sklearn.datasets.load_breast_cancer()).\n",
        "#2. Apply PCA and plot the scree plot (explained variance ratio).\n",
        "#3. Retain 95% variance and transform the dataset.\n",
        "#4. Train KNN on the original data and PCA-transformed data, then compare\n",
        "#accuracy.\n",
        "#5. Visualize the first two principal components using a scatter plot (color by class).\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Standardize (mandatory for PCA & KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. PCA Fit\n",
        "pca = PCA().fit(X_train_scaled)\n",
        "\n",
        "# Scree Plot (Explained Variance)\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"Scree Plot - Breast Cancer Dataset\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 5. Retain 95% Variance\n",
        "pca_95 = PCA(n_components=0.95)\n",
        "X_train_pca = pca_95.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca_95.transform(X_test_scaled)\n",
        "\n",
        "print(\"Number of components for 95% variance:\", pca_95.n_components_)\n",
        "\n",
        "# 6. Train KNN (Original vs PCA)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Original (scaled)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "orig_pred = knn.predict(X_test_scaled)\n",
        "orig_acc = accuracy_score(y_test, orig_pred)\n",
        "\n",
        "# PCA-transformed\n",
        "knn.fit(X_train_pca, y_train)\n",
        "pca_pred = knn.predict(X_test_pca)\n",
        "pca_acc = accuracy_score(y_test, pca_pred)\n",
        "\n",
        "print(\"Accuracy (Original Scaled Data):\", orig_acc)\n",
        "print(\"Accuracy (PCA-Transformed Data):\", pca_acc)\n",
        "\n",
        "# 7. Visualization: First 2 Principal Components\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_2 = pca_2.fit_transform(X_train_scaled)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(X_2[:,0], X_2[:,1], c=y_train, cmap='coolwarm', s=40)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA Visualization (First 2 Components)\")\n",
        "plt.colorbar(label=\"Class\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aLpZ9jrlFgS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9.KNN Regressor with Distance Metrics and K-Value\n",
        "#Analysis\n",
        "#Task:\n",
        "#1. Generate a synthetic regression dataset\n",
        "#(sklearn.datasets.make_regression(n_samples=500, n_features=10)).\n",
        "#2. Train a KNN regressor with:\n",
        "#a. Euclidean distance (K=5)\n",
        "#b. Manhattan distance (K=5)\n",
        "#c. Compare Mean Squared Error (MSE) for both.\n",
        "#3. Test K=1, 5, 10, 20, 50 and plot K vs. MSE to analyze bias-variance tradeoff\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=15, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Test different K values\n",
        "k_values = [1, 4, 10, 20, 50]\n",
        "mse_euclidean = []\n",
        "mse_manhattan = []\n",
        "\n",
        "for k in k_values:\n",
        "    # Euclidean Distance\n",
        "    knn_e = KNeighborsRegressor(n_neighbors=k, metric='euclidean')\n",
        "    knn_e.fit(X_train, y_train)\n",
        "    pred_e = knn_e.predict(X_test)\n",
        "    mse_euclidean.append(mean_squared_error(y_test, pred_e))\n",
        "\n",
        "    # Manhattan Distance\n",
        "    knn_m = KNeighborsRegressor(n_neighbors=k, metric='manhattan')\n",
        "    knn_m.fit(X_train, y_train)\n",
        "    pred_m = knn_m.predict(X_test)\n",
        "    mse_manhattan.append(mean_squared_error(y_test, pred_m))\n",
        "\n",
        "# Print Results\n",
        "print(\"K Values:\", k_values)\n",
        "print(\"MSE Euclidean:\", mse_euclidean)\n",
        "print(\"MSE Manhattan:\", mse_manhattan)\n",
        "\n",
        "# 3. Plot MSE values\n",
        "plt.plot(k_values, mse_euclidean, label=\"Euclidean\")\n",
        "plt.plot(k_values, mse_manhattan, label=\"Manhattan\")\n",
        "plt.xlabel(\"K Value\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.title(\"K vs MSE for Euclidean & Manhattan Distance\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YF_yYhivGqKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.KNN with KD-Tree/Ball Tree, Imputation, and Real-World\n",
        "#Data\n",
        "#Task:\n",
        "#1. Load the Pima Indians Diabetes dataset (contains missing values).\n",
        "#2. Use KNN Imputation (sklearn.impute.KNNImputer) to fill missing values.\n",
        "#3. Train KNN using:\n",
        "#a. Brute-force method\n",
        "#b. KD-Tree\n",
        "#c. Ball Tree\n",
        "#4. Compare their training time and accuracy.\n",
        "#5. Plot the decision boundary for the best-performing method (use 2 most important\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "columns = [\"Preg\", \"Plasma\", \"BP\", \"Skin\", \"Insulin\", \"BMI\", \"Pedigree\", \"Age\", \"Outcome\"]\n",
        "df = pd.read_csv(url, names=columns)\n",
        "\n",
        "# 2. Imputation with KNN\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=columns)\n",
        "\n",
        "X = df_imputed.drop(\"Outcome\", axis=1)\n",
        "y = df_imputed[\"Outcome\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train KNN (KD-Tree)\n",
        "knn_kd = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
        "knn_kd.fit(X_train, y_train)\n",
        "pred_kd = knn_kd.predict(X_test)\n",
        "acc_kd = accuracy_score(y_test, pred_kd)\n",
        "\n",
        "# 4. Train KNN (Ball Tree)\n",
        "knn_ball = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
        "knn_ball.fit(X_train, y_train)\n",
        "pred_ball = knn_ball.predict(X_test)\n",
        "acc_ball = accuracy_score(y_test, pred_ball)\n",
        "\n",
        "print(\"KD-Tree Accuracy:\", acc_kd)\n",
        "print(\"Ball Tree Accuracy:\", acc_ball)\n",
        "\n",
        "# 5. Decision boundary (using 2 important features)\n",
        "feat1 = \"Plasma\"\n",
        "feat2 = \"BMI\"\n",
        "\n",
        "plt.scatter(X_test[feat1], X_test[feat2], c=pred_kd)\n",
        "plt.xlabel(feat1)\n",
        "plt.ylabel(feat2)\n",
        "plt.title(\"Decision Boundary (KD-Tree KNN)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nuwt1MrEHF8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FarFhE4vBzMV"
      },
      "outputs": [],
      "source": []
    }
  ]
}